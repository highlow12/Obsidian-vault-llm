# Obsidian Vault LLM

> Obsidian 볼트와 대화하는 RAG 기반 LLM 채팅 플러그인 – 통합 기능/기술 명세

Obsidian Vault LLM은 볼트의 마크다운 노트를 벡터 임베딩으로 인덱싱하고, LLM과 대화하면서 노트의 내용을 검색(RAG)해 답변을 생성하는 Obsidian 데스크톱 플러그인입니다. 개발자가 바로 구현을 시작할 수 있도록 기능 명세와 기술 명세를 한 문서에 정리했습니다.

---

## 📋 목차

1. [주요 기능](#1-주요-기능)
2. [설치](#2-설치)
3. [초기 설정](#3-초기-설정)
4. [사용 방법](#4-사용-방법)
5. [데이터 저장 위치](#5-데이터-저장-위치)
6. [목표와 성공 지표](#6-목표와-성공-지표)
7. [범위](#7-범위)
8. [주요 사용자 시나리오](#8-주요-사용자-시나리오)
9. [기능 명세](#9-기능-명세)
10. [데이터 흐름](#10-데이터-흐름)
11. [데이터 모델](#11-데이터-모델)
12. [기술 스택 및 프로젝트 구조](#12-기술-스택-및-프로젝트-구조)
13. [프롬프트 설계](#13-프롬프트-설계)
14. [품질·성능·보안 요구](#14-품질성능보안-요구)
15. [테스트 전략](#15-테스트-전략)
16. [구현 현황](#16-구현-현황)
17. [향후 마일스톤](#17-향후-마일스톤)
18. [기여 가이드](#18-기여-가이드)
19. [FAQ](#19-faq)
20. [버전 이력](#20-버전-이력)
21. [라이선스 및 연락처](#21-라이선스-및-연락처)

---

## 1. 주요 기능

### 💬 사이드바 채팅 뷰
- Obsidian 우측 사이드바에 채팅 패널 표시
- **스트리밍 응답**: 토큰 단위 실시간 출력
- **응답 중단**: 생성 중인 응답을 언제든지 중단
- **Ctrl+Enter** 단축키로 메시지 전송
- **소스만 보기**: LLM 응답 없이 RAG 검색 결과(출처 청크)만 표시
- **볼트 검색 답변**: 볼트 인덱스를 검색해 관련 노트 컨텍스트를 LLM에 전달

### 🤖 다중 LLM API 지원

| 제공자 | 설명 |
|-------|------|
| **Google Gemini** | Google Gemini API (기본값, 무료 할당량 제공) |
| **OpenAI 호환** | OpenAI API 및 OpenAI 포맷을 따르는 모든 API |
| **Ollama (로컬)** | 완전 로컬 실행, 인터넷 불필요 |
| **사용자 지정** | 임의의 API URL/모델 직접 입력 |

- Gemini 모델 목록 자동 불러오기 (API 키 입력 후 클릭)
- **제목 생성 전용 모델** 별도 지정 가능
- 시스템 프롬프트 설정

### 📚 벡터 인덱싱 & RAG 검색
- 볼트의 모든 마크다운 파일을 청크 단위로 분할 및 임베딩
- **자동 재인덱싱**: 파일 생성·수정·삭제·이름변경 시 해당 파일만 자동 갱신
- **전체 볼트 임베딩** / **신규 파일만 임베딩** 선택 가능
- 코사인 유사도 기반 Top-K 벡터 검색
- frontmatter, 태그, 내부 링크 파싱
- JSON 파일 기반 로컬 메타데이터/벡터 스토어 (`.obsidian/plugins/obsidian-vault-llm/`)

#### 임베딩 제공자 선택

| 제공자 | 기본 모델 | 설명 |
|-------|---------|------|
| **Google Gemini** | `text-embedding-004` | API 기반 (기본값) |
| **OpenAI** | `text-embedding-3-small` | API 기반 |
| **로컬 (HuggingFace)** | `Xenova/all-MiniLM-L6-v2` | 오프라인 사용 가능 |
| **커스텀 API** | 직접 입력 | OpenAI 호환 임베딩 엔드포인트 |

### 🗂️ 대화 세션 관리
- 대화 세션을 볼트 내 `.ovl/chat-sessions/` 폴더에 JSON으로 자동 저장
- 세션 목록 드롭다운에서 이전 대화 불러오기
- 세션 삭제
- 새 세션 시작

### 💾 대화 저장 (마크다운)
- 진행 중인 대화를 마크다운 노트(.md)로 저장
- 명령 팔레트: **"대화 JSON에서 마크다운 저장"** — 기존 JSON 파일을 마크다운으로 변환
- 기본 저장 폴더 설정 가능

### 🔍 주제 분리 AI (v0.3.0)
대화를 의미론적 경계로 자동 분리하여 주제별로 별도 노트에 저장합니다.

- **슬라이딩 윈도우 유사도 분석**: 인접 턴 간 코사인 유사도로 주제 전환 감지
- **키워드 추출**: 각 주제의 핵심 키워드 자동 추출 (한국어 지원)
- **자동 노트 제목 생성**: 키워드 기반 제목 생성
- **노트 간 자동 링크**: 공통 키워드를 공유하는 주제 간 Obsidian 링크(`[[...]]`) 연결
- **메인 인덱스 노트**: 전체 주제 목록과 각 노트 링크를 담은 인덱스 노트 자동 생성
- frontmatter에 세그먼트 번호, 키워드, 평균 유사도 기록

### 📋 명령 팔레트

| 명령 | 설명 |
|------|------|
| OVL 대화 창 열기 | 사이드바 채팅 패널 열기 |
| 대화 JSON에서 마크다운 저장 | JSON 파일을 마크다운 노트로 변환 |
| 볼트 인덱싱 시작 | 수동으로 전체 볼트 인덱싱 시작 |

---

## 2. 설치

### 수동 설치

1. [Releases](https://github.com/highlow12/Obsidian-vault-llm/releases)에서 최신 버전의 `main.js`, `manifest.json`, `styles.css`를 다운로드합니다.
2. Obsidian 볼트의 플러그인 폴더에 파일을 복사합니다:
   ```
   <볼트 경로>/.obsidian/plugins/obsidian-vault-llm/
   ```
3. Obsidian → 설정 → 커뮤니티 플러그인 → "Obsidian Vault LLM" 활성화

### 소스에서 빌드

```bash
# 의존성 설치
npm install

# 플러그인 빌드 (main.js, styles.css 생성)
npm run build

# 개발 모드 (파일 변경 시 자동 재빌드)
npm run dev

# 테스트 실행
npm test
```

빌드 후 생성된 `main.js`, `manifest.json`, `styles.css`를 플러그인 폴더에 복사하세요.

> **요구사항**: Obsidian 1.5.0 이상, 데스크톱 전용

---

## 3. 초기 설정

### API 설정

Obsidian → 설정 → **OVL 설정** 탭을 엽니다.

| 설정 항목 | 설명 |
|---------|------|
| API 제공사 | Gemini / OpenAI 호환 / Ollama / 사용자 지정 |
| API URL | 채팅 엔드포인트 URL (제공사 선택 시 자동 입력) |
| API 키 | 제공사 API 키 (Gemini: `AIza...`, OpenAI: `sk-...`) |
| 모델 | 사용할 LLM 모델명 |
| 제목 생성 모델 | 세션 제목 자동 생성에 사용할 모델 (비워두면 기본 모델 사용) |
| 시스템 프롬프트 | 모든 요청에 포함할 시스템 메시지 |
| 기본 저장 폴더 | 대화를 저장할 기본 폴더 경로 (볼트 기준) |

#### Google Gemini 사용 예시
1. **API 제공사**를 `Google Gemini`로 선택
2. **API 키**에 [Google AI Studio](https://aistudio.google.com/) 키 입력
3. **Gemini 모델 목록 불러오기** 버튼 클릭 → 드롭다운에서 모델 선택

#### Ollama (로컬) 사용 예시
1. [Ollama](https://ollama.com)를 설치하고 원하는 모델을 pull: `ollama pull llama3.1`
2. **API 제공사**를 `Ollama (로컬)`로 선택 (URL: `http://localhost:11434/v1/chat/completions`)
3. **모델** 항목에 `llama3.1` 입력

### 벡터 인덱싱 설정 (RAG 사용 시)

| 설정 항목 | 기본값 | 설명 |
|---------|-------|------|
| 인덱싱 활성화 | OFF | 볼트 인덱싱 및 벡터 검색 활성화 |
| 청크 크기 | 400 | 각 청크의 최대 토큰 수 |
| 청크 오버랩 | 50 | 인접 청크 간 중복 토큰 수 |
| 검색 결과 수 (Top-K) | 8 | 검색 시 반환할 최대 결과 수 |
| 검색 유사도 임계값 | 0.35 | RAG 검색 결과로 채택할 최소 유사도 (0~1) |
| 저장 유사도 임계값 | 0.75 | 주제 분리 기준 유사도 (0~1) |

### 임베딩 설정

| 설정 항목 | 설명 |
|---------|------|
| 임베딩 제공자 | Gemini / OpenAI / 로컬(HuggingFace) / 커스텀 |
| 임베딩 API URL | 임베딩 엔드포인트 (비워두면 제공자 기본값 사용) |
| 임베딩 API 키 | 임베딩 API 키 (비워두면 LLM API 키 사용) |
| 임베딩 모델 | 사용할 임베딩 모델명 |

---

## 4. 사용 방법

### 채팅 시작

1. 좌측 리본의 **메시지 아이콘** 클릭 또는 명령 팔레트에서 **"OVL 대화 창 열기"** 실행
2. 메시지 입력창에 질문 입력 후 **전송** 버튼 또는 **Ctrl+Enter**

### RAG 검색 답변

인덱싱이 활성화된 상태에서:
- **볼트 검색 답변** 버튼: 볼트 노트를 검색해 관련 컨텍스트와 함께 LLM에 질의
- **소스만 보기** 체크박스: LLM 응답 없이 검색 결과(출처 청크) 목록만 표시

### 전체 볼트 인덱싱

설정 탭에서:
- **전체 임베딩 시작**: 볼트의 모든 노트를 임베딩 (처음 사용 시 또는 임베딩 모델 변경 후 실행)
- **신규 임베딩 시작**: 마지막 인덱싱 이후 변경된 파일만 임베딩

또는 명령 팔레트에서 **"볼트 인덱싱 시작"** 실행.

> 파일을 수정하면 저장 시 자동으로 해당 파일만 재인덱싱됩니다.

### 대화 저장

채팅 패널 상단의 **저장** 버튼:
- 현재 대화를 마크다운 노트로 볼트에 저장
- 세션 제목 입력창에서 파일명 지정

**주제 분리 저장**:
- 저장 시 주제 분리 AI가 작동하여 의미적으로 다른 주제를 자동 감지
- 각 주제가 별도 노트로 저장되고 서로 링크됨 (4턴 이상 대화 권장)
- 전체 주제 목록을 담은 인덱스 노트도 함께 생성

### 세션 관리

채팅 패널 상단:
- **세션 드롭다운**: 이전 대화 세션 선택 및 불러오기
- **새 세션**: 새 대화 시작
- **x 버튼**: 선택한 세션 삭제

### 주제 분리 저장 예시

**시나리오**: 여러 주제가 섞인 긴 대화

```
👤 사용자: 프로젝트 일정을 확인하고 싶어요
🤖 어시스턴트: 프로젝트는 3월 15일까지 완료 예정입니다.

👤 사용자: 그럼 점심 메뉴 추천해줄래요?
🤖 어시스턴트: 오늘은 파스타나 비빔밥이 좋을 것 같습니다.

👤 사용자: 다시 프로젝트로 돌아가서, 마일스톤은?
🤖 어시스턴트: 주요 마일스톤은 다음과 같습니다...
```

**저장 버튼 클릭 시**: AI가 자동으로 주제를 감지해 개별 노트로 저장하고 링크를 생성합니다.

---

## 5. 데이터 저장 위치

| 데이터 | 저장 경로 |
|-------|---------|
| 플러그인 설정 | `.obsidian/plugins/obsidian-vault-llm/data.json` |
| 메타데이터 스토어 | `.obsidian/plugins/obsidian-vault-llm/meta.json` |
| 벡터 스토어 | `.obsidian/plugins/obsidian-vault-llm/vectors.json` |
| 채팅 세션 | `.ovl/chat-sessions/<세션명>.json` |
| 저장된 대화 노트 | 설정한 기본 저장 폴더 또는 볼트 루트 |
| 에러 로그 | `.obsidian/plugins/obsidian-vault-llm/error.log` |

---

## 6. 목표와 성공 지표

- 자연어로 볼트 내용을 질문하면 근거가 표시된 답변을 제공한다:
  - **Warm 캐시** (벡터 스토어 및 LLM 커넥션 준비 완료 상태): **3초 이내**
  - **Cold 스타트** (초기 로드 포함): **5초 이하**
- 새/수정된 노트는 **10초 이내**에 재색인되어 검색 가능해야 한다.
- 모든 데이터는 기본적으로 **로컬에 저장**되며, 외부 전송은 사용자가 명시적으로 설정한 LLM API로만 이뤄진다.

---

## 7. 범위

### 포함
- Obsidian 볼트 내 마크다운(.md) 파일 및 메타데이터(frontmatter, 태그, 링크) 색인
- API 기반 또는 로컬 모델을 통한 임베딩 생성 및 벡터 검색
- 대화형 질의응답(챗), 노트 요약, 새 노트 초안 생성
- 답변 내 출처(파일 경로/헤더/문맥 스니펫) 표시
- 간단한 필터(폴더, 태그)와 개인/워크스페이스 설정 관리

### 제외 (차후 고려)
- 동시 편집/협업 기능
- 멀티미디어 파일 처리(이미지/음성)
- 클라우드 싱크/호스팅

---

## 8. 주요 사용자 시나리오

1. 플러그인 설정에서 LLM API 키와 임베딩 제공자를 설정한다.
2. "전체 임베딩 시작"으로 볼트를 스캔해 임베딩을 생성하고 로컬 JSON 스토어에 저장한다.
3. 사용자가 "지난주 회의록 요약해줘"라고 질의하면, 관련 노트의 스니펫과 함께 답변을 받는다.
4. 사용자가 답변을 바탕으로 "저장"을 선택하면 지정 폴더에 마크다운 노트(.md)가 생성된다.
5. 노트를 수정하면 자동으로 해당 파일만 재색인된다.

---

## 9. 기능 명세

### 9.1 인덱싱/동기화
- 첫 실행 시 전체 스캔 → 문서 단위 → 헤더/문단 기준 청킹 (기본 400 토큰)
- 변경 감지: 파일 생성/수정/삭제/이름변경 이벤트(watch) → 해당 파일만 재임베딩/삭제
- 100ms 디바운스로 연속 수정 시 효율적 처리
- 임베딩 모델 변경 감지 시 기존 인덱스 자동 초기화

### 9.2 검색/QA
- 벡터 검색 Top-K(기본 8) — 향후 폴더, 태그, frontmatter 필터 추가 예정
- 답변에 근거 목록(파일명, 헤더, 스니펫) 포함
- "소스만 보기"(검색 결과만 반환) vs "LLM 답변" 옵션

### 9.3 대화 모드
- 세션 ID로 컨텍스트 유지
- 대화 내용 자동 세션 파일 저장 및 불러오기

### 9.4 노트 생성/저장
- 대화를 마크다운으로 변환하여 볼트에 저장
- 자동 폴더 생성 및 중복 파일명 방지
- JSON 대화 파일 → 마크다운 변환 CLI 명령 (`대화 JSON에서 마크다운 저장`)

### 9.5 주제 분리 AI
- 4턴 이상 대화에서 자동 작동
- 단일 주제 감지 시 일반 저장 수행
- 다중 주제 감지 시 자동으로 분리 저장

### 9.6 설정/보안
- API 키는 Obsidian 플러그인 데이터 파일에 저장
- 모든 볼트 데이터는 로컬에만 저장

---

## 10. 데이터 흐름

### 색인 파이프라인
```
파일 변경
  → 파서(frontmatter, 링크, 본문 추출)
  → 청크 분할
  → 임베딩 생성
  → 메타데이터/벡터를 JSON 스토어에 저장
  → 상태 플래그 업데이트
```

### 질의응답 흐름
```
사용자 프롬프트
  → 쿼리 임베딩 생성
  → 벡터 검색 (Top-K)
  → 컨텍스트 패킹 (유사도 임계값 필터링)
  → LLM 호출 (system prompt + retrieval context)
  → 스트리밍 응답 반환
  → (옵션) 노트로 저장
```

### 주제 분리 흐름
```
대화 저장 요청
  → 각 턴 임베딩 생성
  → 슬라이딩 윈도우 코사인 유사도 계산
  → 주제 경계 탐지 (임계값 미만 또는 급격한 유사도 하락)
  → 세그먼트 생성 및 키워드 추출
  → 개별 노트 저장 + 링크 생성 + 인덱스 노트 생성
```

---

## 11. 데이터 모델

- **Note**: `id`, `path`, `title`, `tags`, `links`, `frontmatter(json)`, `updated_at`, `hash`
- **Chunk**: `id`, `note_id`, `text`, `position`, `token_count`, `section`
- **Embedding**: `chunk_id`, `vector(float[])`
- **Conversation**: `session_id`, `turns[]`, `updated_at`
- **ConversationTurn**: `role`, `content`, `timestamp`, `generationLog`

---

## 12. 기술 스택 및 프로젝트 구조

### 핵심 기술

| 항목 | 내용 |
|-----|------|
| 언어 | TypeScript 5.9+ |
| 플랫폼 | Obsidian Plugin API 1.12+ |
| 빌드 | esbuild |
| 테스트 | Node.js 내장 테스트 러너 |

### 주요 라이브러리

| 라이브러리 | 버전 | 용도 |
|-----------|-----|------|
| `@google/generative-ai` | ^0.24.1 | Gemini API 통합 |
| `@xenova/transformers` | ^2.17.2 | 로컬 HuggingFace 임베딩 |
| `chokidar` | ^5.0.0 | 파일 시스템 감시 |
| `gray-matter` | ^4.0.3 | frontmatter 파싱 |

### 프로젝트 구조

```
src/
├── main.ts                  # 플러그인 진입점 (Obsidian Plugin 클래스)
├── api.ts                   # LLM API 클라이언트 (Gemini, OpenAI, 스트리밍)
├── settings.ts              # 설정 탭 UI
├── types.ts                 # 타입 정의 및 기본값
├── conversation.ts          # 대화 턴 타입 및 마크다운 변환
├── conversationStore.ts     # 대화 → 마크다운 파일 저장
├── chatSessionStore.ts      # 채팅 세션 JSON 저장/불러오기
├── parseTurns.ts            # JSON → 대화 턴 파싱
├── pluginApi.ts             # 플러그인 내부 API 타입
├── logging.ts               # 에러 로그 기록
├── vaultWatcher.ts          # 파일 변경 감지 및 자동 인덱싱
├── ovl.ts                   # CLI 도구 (ovl save-conversation 명령: JSON→마크다운 변환)
├── views/
│   └── chatView.ts          # 사이드바 채팅 뷰 UI
├── modals/
│   └── saveConversationModal.ts  # 대화 저장 모달
├── indexing/
│   ├── indexer.ts           # 파일 인덱싱 통합 (파싱→청킹→임베딩→저장)
│   ├── embeddings.ts        # 임베딩 생성기 (Gemini/OpenAI/로컬/커스텀)
│   ├── chunker.ts           # 텍스트 청크 분할
│   ├── parser.ts            # 마크다운 파싱 (frontmatter, 링크, 태그)
│   ├── metadataStore.ts     # 노트/청크 메타데이터 JSON 스토어
│   ├── vectorStore.ts       # 벡터 JSON 스토어
│   ├── vectorIndex.ts       # 벡터 인덱스 인터페이스
│   ├── vectorIndexFactory.ts # 벡터 인덱스 팩토리
│   ├── queryKeywordEmbedding.ts # 쿼리 임베딩 텍스트 빌더
│   └── types.ts             # 인덱싱 타입 정의
└── topicSeparation/
    ├── index.ts             # 주제 분리 모듈 진입점
    ├── topicSeparationEngine.ts  # 주제 분리 엔진 (유사도 계산, 경계 탐지)
    ├── embeddingService.ts  # 임베딩 생성 및 코사인 유사도
    ├── keywordExtractor.ts  # 키워드 추출
    ├── multiNoteSaver.ts    # 세그먼트 → 다중 노트 저장
    └── types.ts             # 주제 분리 타입 정의
```

### 코드 품질 지표

| 항목 | 현재 | 목표 |
|-----|------|------|
| 총 코드 라인 수 | ~2,700 라인 | - |
| TypeScript 커버리지 | 100% | 유지 |
| 테스트 커버리지 | ~40% | 80% 이상 |
| 빌드 시간 | ~2초 | 5초 이하 |
| 플러그인 번들 크기 | ~1.0MB | 1.5MB 이하 |

### 저장소 구성

- **메타데이터 스토어**: JSON 파일 (`meta.json`) — 인덱스 시그니처 기반 모델 변경 감지
- **벡터 스토어**: JSON 파일 (`vectors.json`) — 코사인 유사도 선형 검색  
  (향후: FAISS/hnswlib 도입으로 ANN 검색 지원 예정)

---

## 13. 프롬프트 설계

- **System Prompt**: "너는 Obsidian 볼트의 전문 리서치 어시스턴트다. 항상 출처를 마크다운으로 인용하고, 모르는 내용은 추측하지 않는다."
- **Retrieval Prompt 템플릿**:
  - 입력: 사용자 질문, 검색된 컨텍스트 리스트(파일명/섹션/본문), 지시사항(길이, 톤)
  - 출력: 마크다운 답변 + 근거 각주 또는 인라인 블록
- **요약 Prompt**: 헤더별 요약, 액션 아이템, 다음 질문 제안.
- **노트 생성 Prompt**: 템플릿 채우기(제목, 요약, 인용).

---

## 14. 품질·성능·보안 요구

| 항목 | 요구 사항 |
|-----|---------|
| **응답 속도** | 질의 응답 총 소요 ≤ 3초 (캐시 기준) |
| **색인 TPS** | 목표 20~30 chunk/초 (M1 Pro, 로컬 임베딩, chunk_size 400 기준) |
| **신뢰성** | 작업 큐 재시도, 부분 실패 시 로그와 상태 플래그 유지 |
| **보안/프라이버시** | 로컬 우선, 외부 전송 시 명시적 동의 및 최소 데이터 전송 |
| **접근성** | API는 JSON 스키마 제공 |

### 성능 벤치마크 (참고, M1 Pro 10코어)

| 작업 | 현재 성능 | 목표 |
|-----|---------|------|
| 파일 파싱 | ~50 파일/초 | 유지 |
| 청킹 | ~100 청크/초 | 유지 |
| 로컬 임베딩 | ~20 청크/초 | 30 청크/초 |
| Gemini 임베딩 | ~50 청크/초 | 100 청크/초 |
| 벡터 검색 (100 파일) | ~50ms | 유지 |
| 벡터 검색 (1000 파일) | ~500ms | 100ms 이하 |
| LLM 응답 | ~2~3초 (캐시) | 유지 |

### 알려진 기술적 부채

1. **벡터 검색 성능**: 현재 선형 검색 O(n) → FAISS/hnswlib ANN 도입 예정
2. **임베딩 생성 속도**: 순차 처리 → 배치 처리 및 병렬화 예정
3. **메모리 사용량**: 전체 임베딩 메모리 로드 → 온디맨드 로딩 및 LRU 캐시 예정
4. **테스트 커버리지**: 기본 기능만 테스트됨 → 80% 이상 목표
5. **에러 처리**: 체계적인 에러 분류 및 복구 전략 필요

---

## 15. 테스트 전략

- **단위 테스트**: 파서(프론트매터/링크), 청크 로직, 필터 적용, 프롬프트 빌더
- **통합 테스트**: 소형 볼트로 색인→검색→응답 경로 검증, 삭제/수정 이벤트 처리
- **수동 테스트**: Obsidian 플러그인 UI와의 왕복, 응답 출처 정확도 샘플링

```bash
npm test
```

---

## 16. 구현 현황

### 현재 진행 상황 (v0.3.0)

| 영역 | 진행률 | 상태 |
|------|--------|------|
| **기본 인프라** | 100% | ✅ 완료 |
| **플러그인 통합** | 100% | ✅ 완료 |
| **채팅 UI** | 100% | ✅ 완료 |
| **LLM API 통합 (스트리밍)** | 100% | ✅ 완료 |
| **대화 세션 저장** | 100% | ✅ 완료 |
| **파일 인덱싱** | 100% | ✅ 완료 |
| **벡터 검색** | 100% | ✅ 완료 |
| **RAG QA** | 100% | ✅ 완료 |
| **주제 분리 AI** | 100% | ✅ 완료 (v0.3.0) |
| **검색 필터 (폴더/태그)** | 0% | 🔜 Phase 2 예정 |
| **성능 최적화 (ANN)** | 0% | 🔜 Phase 2 예정 |

### 완료된 기능 상세

#### ✅ Obsidian 플러그인 통합
- 플러그인 초기화 및 생명주기 관리 (Obsidian Plugin API 완전 통합)
- 설정 탭 UI, 리본 아이콘 (message-circle)
- 명령 팔레트: OVL 대화 창 열기 / 대화 JSON에서 마크다운 저장 / 볼트 인덱싱 시작

#### ✅ 실시간 채팅 인터페이스
- 사이드바에 통합된 채팅 UI
- 실시간 스트리밍 응답 (토큰 단위)
- 응답 중단 기능 (AbortController)
- Ctrl+Enter 메시지 전송
- 세션 드롭다운, 새 세션, 세션 삭제

#### ✅ LLM API 통합
- Google Gemini / OpenAI 호환 / Ollama / 커스텀 API 지원
- 스트리밍 및 비스트리밍 응답 모두 지원
- 토큰 사용량(usage) 추적 및 표시
- 시스템 프롬프트, 제목 생성 전용 모델

#### ✅ 벡터 인덱싱 & RAG
- 마크다운 파싱 (frontmatter, 태그, 내부 링크)
- 청크 분할 (기본 400 토큰, 50 오버랩)
- 임베딩 생성 (Gemini / OpenAI / 로컬 HuggingFace / 커스텀)
- 코사인 유사도 기반 Top-K 검색
- 파일 변경 자동 감지 및 재인덱싱 (100ms 디바운스)
- 임베딩 모델 변경 시 자동 인덱스 초기화

#### ✅ 주제 분리 AI (v0.3.0)
- Gemini Embedding API 기반 턴별 벡터화
- 슬라이딩 윈도우 코사인 유사도로 주제 경계 탐지 (임계값 0.75, 급락 0.15)
- 한국어 키워드 추출 및 메타데이터 생성
- 주제별 개별 노트 저장 + Obsidian 링크 자동 생성
- 메인 인덱스 노트 생성

---

## 17. 향후 마일스톤

### 🚀 Phase 2: 검색 고도화 및 성능 최적화

#### 우선순위 1: 검색 필터 구현
- [ ] 폴더 기반 필터 (특정 폴더만 검색, 폴더 제외)
- [ ] 태그 필터 (다중 태그 AND/OR 조건)
- [ ] Frontmatter 필터 (키/값, 날짜 범위)

#### 우선순위 2: 성능 최적화
- [ ] ANN(Approximate Nearest Neighbor) 알고리즘 도입 (FAISS 또는 hnswlib)
- [ ] 배치 임베딩 처리
- [ ] 온디맨드 임베딩 로드 및 LRU 캐시
- [ ] 대용량 파일 처리 개선

#### 우선순위 3: UX 개선
- [ ] 대화 컨텍스트 관리 (최근 N턴 자동 요약 및 메모리 압축)
- [ ] 노트 요약 및 액션 아이템 추출
- [ ] Obsidian 에디터 내 인라인 AI 어시스턴트
- [ ] 우클릭 컨텍스트 메뉴 통합

#### 우선순위 4: 추가 LLM 지원
- [ ] Claude (Anthropic) 지원
- [ ] 작업별 모델 자동 선택 (요약용/대화용/검색용)

**Phase 2 총 예상 완료: 3~4개월**

---

### 🌟 Phase 3: 고급 AI 기능 (장기 목표)

- [ ] 의미적으로 관련된 노트 자동 링크 제안 및 그래프 뷰 통합
- [ ] 하이브리드 검색 (키워드 + 벡터)
- [ ] 멀티모달 지원 (이미지 OCR, PDF 파싱)
- [ ] Obsidian 플러그인 연동 (캘린더, 태스크, Dataview)
- [ ] 다중 볼트 지원

**Phase 3 총 예상 완료: 8~12개월**

---

### 🔒 Phase 4: 프로덕션 준비 (지속 개선)

- [ ] OS Keychain 통합 (API 키 보안 강화)
- [ ] 단위 테스트 커버리지 > 80%
- [ ] CI/CD 파이프라인 자동화
- [ ] 한국어/영어 사용자 가이드 및 비디오 튜토리얼
- [ ] 자동 버전 관리 및 마이그레이션 스크립트

---

## 18. 기여 가이드

### 기여 우선순위

#### 🔥 긴급 (Phase 2)
1. **검색 필터 구현** - 폴더, 태그, frontmatter 필터
2. **성능 최적화** - ANN 알고리즘 도입, 배치 처리
3. **테스트 커버리지 향상** - 80% 목표

#### 🚀 중요 (Phase 2-3)
4. **대화 컨텍스트 관리** - 다중 턴 대화 지원
5. **노트 요약 및 생성** - AI 기반 노트 관리
6. **UI/UX 개선** - 인라인 어시스턴트, 컨텍스트 메뉴

#### 💡 아이디어 환영 (Phase 3+)
7. **멀티모달 지원** - 이미지, PDF 처리
8. **플러그인 생태계** - 다른 Obsidian 플러그인 연동
9. **협업 기능** - 공유 볼트, 팀 템플릿

### 기여 방법
1. **이슈 생성**: 기여하고 싶은 기능이나 버그를 이슈로 등록
2. **논의**: 구현 방법 및 범위를 maintainer와 논의
3. **개발**: Fork 후 feature 브랜치에서 개발
4. **테스트**: 기존 테스트 통과 및 새 테스트 추가
5. **PR 제출**: 상세한 설명과 함께 Pull Request 제출

### 코딩 가이드라인
- TypeScript 엄격 모드 준수
- 한국어 주석 권장 (영어도 가능)
- 기존 코드 스타일 유지
- 모든 public 함수에 JSDoc 추가
- 테스트 코드 필수

---

## 19. FAQ

**Q: 이 플러그인을 사용하려면 유료 API가 필요한가요?**  
A: LLM API 키가 필요합니다. Gemini (무료 할당량 있음), OpenAI, 또는 로컬 Ollama를 사용할 수 있습니다.

**Q: 오프라인에서도 사용할 수 있나요?**  
A: 로컬 임베딩 모델(HuggingFace) + Ollama 조합으로 완전 오프라인 사용이 가능합니다.

**Q: 내 데이터는 안전한가요?**  
A: 모든 데이터는 로컬에 저장되며, LLM API에는 필요한 컨텍스트만 전송됩니다.

**Q: 초기 인덱싱에 얼마나 걸리나요?**  
A: 100개 파일은 약 1~2분, 1000개 파일은 10~20분 정도 소요됩니다.

**Q: 검색이 너무 느려요.**  
A: 현재 대용량 볼트(1000개 이상)에서는 성능 이슈가 있습니다. Phase 2에서 ANN 알고리즘 도입으로 개선 예정입니다.

**Q: 주제 분리가 제대로 안 돼요.**  
A: 4턴 이상의 대화에서만 작동하며, 의미적으로 구분되는 주제가 있어야 합니다. 짧거나 단일 주제 대화는 분리되지 않습니다.

**Q: 인덱싱이 실패합니다.**  
A: API 키가 올바른지 확인하고, 개발자 도구(Ctrl/Cmd+Shift+I) 콘솔 로그를 확인하세요.

---

## 20. 버전 이력

### v0.3.0
- ✨ **주제 분리 AI** 추가: 대화를 의미론적 주제별로 자동 분리하여 별도 노트 저장
- ✨ 키워드 기반 자동 노트 제목 생성
- ✨ 주제 간 자동 Obsidian 링크 생성
- ✨ 전체 주제 인덱스 노트 자동 생성

### v0.2.0
- ✨ 벡터 인덱싱 시스템 구현 (메타데이터/벡터 JSON 스토어)
- ✨ RAG 기반 볼트 검색 및 LLM 답변
- ✨ 파일 변경 시 자동 재인덱싱

### v0.1.0
- ✨ Obsidian 플러그인 기본 구조
- ✨ LLM API 통합 (Gemini, OpenAI 호환, Ollama)
- ✨ 스트리밍 응답 지원
- ✨ 사이드바 채팅 UI
- ✨ 대화 세션 저장/불러오기
- ✨ 대화 마크다운 저장

---

## 21. 라이선스 및 연락처

이 프로젝트는 현재 개발 중이며 라이선스가 명시되지 않았습니다. 사용 전 저작권자에게 문의하세요.

- **GitHub**: [highlow12/Obsidian-vault-llm](https://github.com/highlow12/Obsidian-vault-llm)
- **이슈**: [GitHub Issues](https://github.com/highlow12/Obsidian-vault-llm/issues)

이 문서는 개발 진행 상황을 반영하여 지속적으로 업데이트됩니다.
