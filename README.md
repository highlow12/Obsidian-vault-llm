# Obsidian Vault LLM – 통합 기능/기술 명세(PRD)

## Obsidian 플러그인 실행

이 저장소는 Obsidian 데스크톱용 플러그인으로 빌드할 수 있습니다.

```bash
npm install
npm run build
```

생성되는 파일:
- `main.js`
- `manifest.json`
- `styles.css`

Obsidian의 플러그인 폴더에 위 파일을 복사하면 플러그인이 로드됩니다.

개발자가 바로 구현을 시작할 수 있도록 기능 명세와 기술 명세를 한 문서에 정리했습니다. Obsidian 볼트의 마크다운 노트를 LLM이 검색·요약·생성할 수 있도록 하는 로컬 우선(Local-first) 도구를 목표로 합니다.

## GitHub Copilot 에이전트 한글 출력 설정

GitHub Copilot 에이전트가 작업 중 모든 설명과 메시지를 한국어로 출력하도록 하려면:

1. **`.github/copilot-instructions.md` 파일 생성**: 리포지토리 루트에 `.github` 디렉토리를 만들고 `copilot-instructions.md` 파일을 추가합니다.

2. **한국어 출력 지시사항 추가**: 파일에 다음 내용을 작성합니다:
   ```markdown
   # Copilot Instructions
   
   - 모든 응답, 설명, 커밋 메시지를 한국어로 작성하세요.
   - Write all responses, descriptions, and commit messages in Korean.
   - 코드 작성 시 주석도 한국어로 작성하세요.
   ```

3. **Copilot 대화 시작 시 명시**: Copilot과 대화를 시작할 때 "한국어로 답변해 주세요" 또는 "Please respond in Korean"이라고 요청합니다.

4. **워크스페이스 설정 파일 활용**: `.vscode/settings.json`에 다음을 추가할 수 있습니다:
   ```json
   {
     "github.copilot.advanced": {
       "language": "ko"
     }
   }
   ```

참고: GitHub Copilot의 언어 설정은 프롬프트 기반이므로, 대화 시작 시 명시적으로 한국어 사용을 요청하는 것이 가장 효과적입니다.

## 1. 목표와 성공 지표
- 자연어로 볼트 내용을 질문하면 근거가 표시된 답변을 3초 이내에 제공한다 (Top-K 검색 + LLM 응답, warm 캐시=프로세스/벡터 스토어/LLM 커넥션이 이미 준비된 상태 / cold 스타트=초기 로드 포함, 목표 5초 이하).
- 새/수정된 노트는 10초 이내에 재색인되어 검색 가능해야 한다.
- 모든 데이터는 기본적으로 로컬에 저장되며, 외부 전송은 사용자가 명시적으로 설정한 LLM API로만 이뤄진다.

## 2. 범위
### 포함
- Obsidian 볼트 내 마크다운(.md) 파일 및 메타데이터(frontmatter, 태그, 링크) 색인
- 로컬 백그라운드 워커/CLI를 통한 임베딩 생성 및 벡터 검색
- 대화형 질의응답(챗), 노트 요약, 새 노트 초안 생성
- 답변 내 출처(파일 경로/헤더/문맥 스니펫) 표시
- 간단한 필터(폴더, 태그)와 개인/워크스페이스 설정 관리

### 제외(차후 고려)
- 동시 편집/협업 기능
- 멀티미디어 파일 처리(이미지/음성)
- 클라우드 싱크/호스팅

## 3. 주요 사용자 시나리오
1) 사용자는 `ovl init`으로 볼트 경로와 LLM API 키를 설정한다.  
2) 워커가 볼트를 스캔해 임베딩을 생성하고 로컬 DB/벡터 스토어에 저장한다.  
3) 사용자가 “지난주 회의록 요약해줘”라고 질의하면, 관련 노트의 스니펫과 함께 답변을 받는다.  
4) 사용자가 답변을 바탕으로 “새 노트로 저장”을 선택하면 지정 폴더에 초안(.md)이 생성된다.  
5) 노트를 수정하면 워커가 변경을 감지하고 해당 파일만 재색인한다.

## 4. 기능 명세(Functional Spec)
1. **초기 설정**
   - CLI 명령: `ovl init --vault <path> --provider openai --api-key <key>`  
   - 설정 파일: `~/.ovl/config.yaml` (볼트 경로, LLM 설정, 필터 규칙, 기본 K 값)
2. **인덱싱/동기화**
   - 첫 실행 시 전체 스캔 → 문서 단위 → 헤더/문단 기준 chunking (기본 300~500 토큰)
   - 변경 감지: 파일 생성/수정/삭제 이벤트(watch) → 해당 파일만 재임베딩/삭제
   - 재시도 큐: 임베딩 실패 시 지수 백오프(초기 1초, 배수 2x) 후 재시도, 최대 3회(기본값) 후 스킵 로그
3. **검색/QA**
   - 벡터 검색 Top-K(기본 8) + 필터(폴더, 태그, frontmatter 키/값)
   - 답변에는 근거 리스트(파일명, 헤더, 스니펫, 토큰 오프셋) 포함
   - 옵션: “소스만 보기”(검색 결과만 반환) vs “LLM 답변”
4. **대화 모드**
   - 세션 ID로 컨텍스트 유지, 최근 10턴(기본값)을 요약해 메모리에 압축
   - 인용 마크다운(각주 또는 각 소스 블록) 형태로 출처 삽입
5. **노트 생성/요약**
   - “새 노트로 저장” 시 템플릿 적용(폴더, 파일명 규칙 `YYYY-MM-DD-title.md`)
   - 기존 노트 요약: 헤더별 요약 + 액션 아이템 추출, frontmatter에 `last_summary_at` 기록
6. **설정/보안**
   - API 키는 OS 보안 스토리지 또는 로컬 암호화 파일에 저장
   - 로그/메트릭 최소화(기본 비활성), 사용자가 opt-in 시 익명 통계 전송

## 5. 데이터 흐름
1) **색인 파이프라인**  
   파일 변경 → 파서(frontmatter, 링크, 본문 추출) → 청크 분할 → 임베딩 생성 →  
   메타데이터/임베딩을 SQLite + 벡터 스토어에 저장 → 상태 플래그 업데이트.
2) **질의응답 흐름**  
   사용자 프롬프트 → 파라메터(K, 필터) 확인 → 벡터 검색 → 컨텍스트 패킹 →  
   LLM 호출(system + retrieval prompt) → 답변과 출처 마크다운 반환 → (옵션) 노트로 저장.

## 6. 데이터 모델(요약)
- **Note**: `id`, `path`, `title`, `tags`, `links`, `frontmatter(json)`, `updated_at`, `hash`
- **Chunk**: `id`, `note_id`, `text`, `embedding`, `position`, `token_count`, `section`
- **Conversation**: `session_id`, `turn`, `role`, `content`, `summary_cache`
- **Settings**: `vault_path`, `provider`, `api_key_ref`, `default_k`, `chunk_size`, `filters`

## 7. 저장소/컴포넌트
- **구현 언어**: TypeScript(Node.js) 기준으로 개발
- **파일 시스템**: Obsidian 볼트 원본(.md)
- **메타데이터 DB**: SQLite (로컬 `~/.ovl/meta.db`)
- **벡터 스토어**: 로컬 우선, 향후 원격 플러그 가능  
  - Chroma: 지속성, 멀티프로세스 접근, 디스크/원격 등 백엔드를 교체할 수 있는 플러그블 스토리지가 필요할 때 기본값  
  - FAISS: 메모리 사용을 줄인 단일 프로세스, 최대 성능이 필요할 때
- **서비스 구성**:  
  - `ovl daemon`: 파일 감시 + 색인 워커  
  - `ovl query`: CLI 질의/챗, 또는 로컬 HTTP API(`localhost:11434` 등)  
  - (옵션) Obsidian 플러그인: 로컬 API 연동 UI

## 8. 프롬프트 설계(초안)
- **System Prompt**: “너는 Obsidian 볼트의 전문 리서치 어시스턴트다. 항상 출처를 마크다운으로 인용하고, 모르는 내용은 추측하지 않는다.”
- **Retrieval Prompt 템플릿**:  
  - 입력: 사용자 질문, 검색된 컨텍스트 리스트(파일명/섹션/본문), 지시사항(길이, 톤)  
  - 출력: 마크다운 답변 + 근거 각주 또는 인라인 블록
- **요약 Prompt**: 헤더별 요약, 액션 아이템, 다음 질문 제안.
- **노트 생성 Prompt**: 템플릿 채우기(제목, 요약, 인용).

## 9. 품질/성능/보안 요구
- **성능**: 색인 TPS 목표 20~30 chunk/초(예: M1 Pro 10코어 CPU, CPU-only, 로컬 임베딩 HF all-MiniLM-L6-v2, chunk_size 400 기준의 참조 값이며, 하위 사양에서는 비례 감소 허용·설정으로 조정), 질의 응답 총 소요 ≤ 3초(캐시/HF 모델 사용 시)  
- **신뢰성**: 작업 큐 재시도, 부분 실패 시 로그와 상태 플래그 유지  
- **보안/프라이버시**: 로컬 우선, 외부 전송 시 명시적 동의 및 최소 데이터 전송; API 키 암호화 저장
- **접근성**: CLI는 기본 텍스트 출력, API는 JSON 스키마 제공

## 10. 테스트 전략
- **단위 테스트**: 파서(프론트매터/링크), 청크 로직, 필터 적용, 프롬프트 빌더
- **통합 테스트**: 소형 볼트로 색인→검색→응답 경로 검증, 삭제/수정 이벤트 처리
- **수동 테스트**: Obsidian 플러그인 UI와의 왕복, 응답 출처 정확도 샘플링

## 11. 구현 상태 및 기능 목록

### ✅ 완료된 기능 (v0.1.0)

#### 1. Obsidian 플러그인 통합 ✅
- **플러그인 초기화 및 생명주기 관리**
  - Obsidian 플러그인 API 완전 통합
  - 설정 탭 UI 제공
  - 리본 아이콘 추가 (message-circle)
  
- **커맨드 팔레트 통합**
  - "OVL 대화 창 열기" 커맨드
  - "대화 JSON에서 마크다운 저장" 커맨드

#### 2. 실시간 채팅 인터페이스 ✅
- **ChatView 구현**
  - 사이드바에 통합된 채팅 UI
  - 실시간 메시지 입력 및 AI 응답 표시
  - Ctrl+Enter로 메시지 전송
  - 세션 ID 자동 생성 (`session-TIMESTAMP`)
  - 타임스탬프 표시
  - 대화 저장 버튼
  - 로딩 상태 UI 처리

#### 3. LLM API 통합 ✅
- **다중 제공사 지원**
  - Google Gemini (`@google/genai` 라이브러리 사용)
  - OpenAI (공식 API)
  - Ollama (로컬 LLM)
  - 커스텀 API 엔드포인트
  
- **API 클라이언트 기능**
  - `requestAssistantReply()`: 대화 턴 기반 AI 응답
  - 시스템 프롬프트 지원
  - Bearer 토큰 자동 처리
  - 에러 로깅 및 처리
  - 응답 형식 자동 파싱

#### 4. 설정 관리 ✅
- **설정 UI**
  - LLM 제공사 선택 (드롭다운)
  - API URL 및 키 입력
  - 모델명 설정
  - 시스템 프롬프트 커스터마이징
  - Gemini 모델 목록 자동 로드
  - 기본 저장 폴더 설정

#### 5. 대화 저장 기능 ✅
- **마크다운 변환**
  - JSON 형식 대화를 마크다운으로 변환
  - 역할별 이모지 표시 (👤 사용자, 🤖 어시스턴트)
  - 타임스탬프 포함
  - 파일명 형식: `YYYY-MM-DD-sessionId.md`
  
- **저장 기능**
  - Obsidian 볼트에 저장
  - 자동 폴더 생성
  - 중복 파일명 방지 (숫자 추가)
  - SaveConversationModal UI 제공

#### 6. CLI 도구 (`ovl`) ✅
- **초기화 명령**
  ```bash
  ovl init --vault /path/to/vault --provider openai --api-key YOUR_KEY
  ```
  - 설정 파일 생성: `~/.ovl/config.yaml`
  - API 키 안전 저장: `~/.ovl/api_key.txt` (600 권한)
  - 볼트 경로 자동 감지
  
- **대화 저장 명령**
  ```bash
  ovl save-conversation --session-id my-session --input turns.json --output /path/to/vault
  ```
  - JSON 파일에서 대화 읽기
  - 마크다운으로 변환 및 저장

#### 7. 에러 로깅 시스템 ✅
- **로깅 인프라**
  - Obsidian 플러그인 로그 통합
  - API 에러 추적
  - 사용자 친화적 에러 메시지

---

### 📋 대화 저장 사용 예시

#### CLI 사용 (JSON → 마크다운)

**입력 형식 (conversation.json)**
```json
[
  {
    "role": "user",
    "content": "Obsidian에서 태그를 어떻게 사용하나요?",
    "timestamp": "2024-02-15T10:00:00Z"
  },
  {
    "role": "assistant",
    "content": "Obsidian에서 태그는 #태그명 형식으로 사용합니다.",
    "timestamp": "2024-02-15T10:00:05Z"
  }
]
```

**실행**
```bash
ovl save-conversation --session-id obsidian-tags --input conversation.json --output ./
```

**출력 (2024-02-15-obsidian-tags.md)**
```markdown
# 대화 기록 - obsidian-tags

생성일: 2024-02-15T10:00:00.000Z

---

## 👤 사용자
*2024-02-15T10:00:00Z*

Obsidian에서 태그를 어떻게 사용하나요?

## 🤖 어시스턴트
*2024-02-15T10:00:05Z*

Obsidian에서 태그는 #태그명 형식으로 사용합니다.
```

---

## 12. 향후 마일스톤

### ✅ Phase 1: MVP 완성 (v0.2.0 - 완료!)
**목표: 기본적인 볼트 검색 및 질의응답 기능 구현**

- [x] **파일 인덱싱 시스템**
  - [x] 볼트 전체 스캔 및 마크다운 파싱
  - [x] 프론트매터, 태그, 링크 추출
  - [x] 파일 변경 감지 (watch) 및 자동 재색인
  - [x] SQLite 메타데이터 저장
  
- [x] **벡터 검색 구현**
  - [x] 로컬 임베딩 생성 (HuggingFace all-MiniLM-L6-v2)
  - [x] 벡터 스토어 통합 (SQLite 기반)
  - [x] 문서 청킹 (300~500 토큰 단위)
  - [x] Top-K 검색 (기본값: 8)
  
- [x] **RAG 기반 질의응답**
  - [x] 벡터 검색 + LLM 통합
  - [x] 컨텍스트 패킹 및 프롬프트 구성
  - [x] 출처 표시 (파일명, 헤더, 스니펫)
  - [x] "소스만 보기" vs "LLM 답변" 옵션
  
- [ ] **검색 필터** (차후 구현)
  - [ ] 폴더 기반 필터
  - [ ] 태그 필터
  - [ ] Frontmatter 키/값 필터
  - [ ] 날짜 범위 필터

**완료일: 2026-02-17**

### 📖 사용 방법 (v0.2.0)

#### 1. 플러그인 설치
```bash
# 의존성 설치
npm install

# 플러그인 빌드
npm run build

# 생성된 파일을 Obsidian 플러그인 폴더에 복사
# - main.js
# - manifest.json
# - styles.css
```

#### 2. 인덱싱 활성화
1. Obsidian 설정 → 커뮤니티 플러그인 → "Obsidian Vault LLM" 활성화
2. 플러그인 설정에서 "인덱싱 활성화" 토글
3. 설정값 조정 (선택사항):
   - 청크 크기: 기본 400 토큰
   - 청크 오버랩: 기본 50 토큰
   - 검색 결과 수: 기본 8

#### 3. 초기 인덱싱 실행
- 커맨드 팔레트 (Ctrl/Cmd + P) 열기
- "OVL: 볼트 인덱싱 시작" 실행
- 진행 상황은 알림으로 표시됨 (10개 파일마다)

#### 4. RAG 기반 대화 사용
1. 리본에서 메시지 아이콘 클릭 또는 "OVL 대화 창 열기" 커맨드 실행
2. "RAG 사용" 체크박스 활성화
3. 질문 입력 (예: "프로젝트 관련 노트를 요약해줘")
4. 옵션:
   - **RAG 사용**: 볼트에서 관련 문서를 검색하여 컨텍스트로 제공
   - **소스만 보기**: 검색 결과만 표시 (LLM 호출 없음)

#### 5. 자동 재색인
- 파일 생성, 수정, 삭제 시 자동으로 인덱스 업데이트
- 100ms 디바운스로 연속 수정 시 효율적 처리

### ⚠️ 알려진 제한사항 (v0.2.0)

1. **성능**: 대규모 볼트(1000개 이상 파일)에서 검색이 느릴 수 있음
   - 원인: 전체 임베딩을 메모리에 로드하여 선형 검색
   - 개선 예정: ANN 알고리즘 또는 전용 벡터 DB 도입

2. **문장 분할**: 약어(Dr., Mr.)나 소수점에서 잘못 분할될 수 있음
   - 개선 예정: 더 정교한 문장 경계 감지

3. **첫 실행**: 임베딩 모델을 HuggingFace에서 다운로드 (약 50MB)
   - 이후 실행은 로컬 캐시 사용

4. **검색 필터**: 아직 미구현 (폴더, 태그, 날짜 필터 등)


---

### 🚀 Phase 2: 고급 기능 (MVP 이후)
**목표: 사용자 경험 향상 및 고급 AI 기능 추가**

- [ ] **대화 컨텍스트 관리**
  - [ ] 대화 세션 컨텍스트 유지
  - [ ] 최근 10턴 자동 요약
  - [ ] 메모리 압축 및 최적화
  - [ ] 대화 히스토리 검색
  
- [ ] **노트 생성 및 요약**
  - [ ] 기존 노트 요약 (헤더별)
  - [ ] 액션 아이템 자동 추출
  - [ ] 템플릿 기반 새 노트 생성
  - [ ] Frontmatter 자동 생성
  
- [ ] **Obsidian UI 통합 강화**
  - [ ] 에디터 내 인라인 AI 어시스턴트
  - [ ] 선택 텍스트 컨텍스트 메뉴
  - [ ] 노트 링크 자동 제안
  - [ ] 미리보기 패널 통합
  
- [ ] **다중 LLM 모델 지원**
  - [ ] Claude (Anthropic)
  - [ ] Llama (Meta)
  - [ ] 로컬 모델 (llama.cpp)
  - [ ] 모델별 프리셋 설정

**예상 완료: 4~6개월**

---

### 🌟 Phase 3: 고급 기능 확장 (장기 목표)
**목표: 멀티모달 및 협업 기능 추가**

- [ ] **멀티모달 지원**
  - [ ] 이미지 인식 (OCR, Vision API)
  - [ ] PDF 파일 처리
  - [ ] 표 및 다이어그램 분석
  - [ ] 음성 메모 전사
  
- [ ] **고급 검색 기능**
  - [ ] 그래프 기반 관계 검색
  - [ ] 시맨틱 유사도 검색
  - [ ] 하이브리드 검색 (키워드 + 벡터)
  - [ ] 시간 기반 트렌드 분석
  
- [ ] **플러그인 생태계**
  - [ ] 캘린더 플러그인 연동
  - [ ] 태스크 플러그인 연동
  - [ ] Dataview 쿼리 지원
  - [ ] 커스텀 뷰 API
  
- [ ] **성능 최적화**
  - [ ] 증분 색인 (incremental indexing)
  - [ ] 캐싱 전략
  - [ ] 병렬 처리
  - [ ] 메모리 사용 최적화
  
- [ ] **다중 볼트 지원**
  - [ ] 프로파일 전환
  - [ ] 볼트 간 검색
  - [ ] 통합 설정 관리

**예상 완료: 8~12개월**

---

### 🔒 Phase 4: 프로덕션 준비 (안정화)
**목표: 보안, 프라이버시, 안정성 강화**

- [ ] **보안 강화**
  - [ ] API 키 암호화 저장 (OS Keychain)
  - [ ] 데이터 익명화 옵션
  - [ ] 감사 로그 (audit log)
  - [ ] 권한 관리
  
- [ ] **프라이버시 보호**
  - [ ] 로컬 전용 모드
  - [ ] 데이터 전송 최소화
  - [ ] Opt-in 원격 분석
  - [ ] GDPR 준수
  
- [ ] **안정성 및 테스트**
  - [ ] 단위 테스트 커버리지 > 80%
  - [ ] 통합 테스트 자동화
  - [ ] E2E 테스트
  - [ ] 성능 벤치마크
  
- [ ] **문서화**
  - [ ] 사용자 가이드
  - [ ] API 문서
  - [ ] 개발자 가이드
  - [ ] 비디오 튜토리얼

**예상 완료: 지속적 개선**

---

## 13. 기여 가이드

현재 프로젝트는 Phase 1(MVP) 단계에 있습니다. 다음 우선순위로 기여를 환영합니다:

1. **파일 인덱싱 시스템** - 가장 높은 우선순위
2. **벡터 검색 구현** - 핵심 기능
3. **RAG 기반 질의응답** - 사용자 가치 제공
4. **테스트 코드 작성** - 안정성 확보

기여 전에 이슈를 생성하여 논의해 주세요.

---

## 14. 현재 진행 상황 요약

| 영역 | 진행률 | 상태 |
|------|--------|------|
| **기본 인프라** | 100% | ✅ 완료 |
| **플러그인 통합** | 100% | ✅ 완료 |
| **채팅 UI** | 100% | ✅ 완료 |
| **LLM API 통합** | 100% | ✅ 완료 |
| **대화 저장** | 100% | ✅ 완료 |
| **CLI 도구** | 100% | ✅ 완료 |
| **파일 인덱싱** | 100% | ✅ 완료 |
| **벡터 검색** | 100% | ✅ 완료 |
| **RAG QA** | 100% | ✅ 완료 |

**현재 버전: v0.2.0**  
**다음 마일스톤: v0.3.0 (검색 필터 및 성능 최적화)**

---

## 15. 개발 환경 설정

### 요구사항
- Node.js 16+
- TypeScript 5+
- Obsidian 1.5.0+

### 설치 및 빌드
```bash
# 의존성 설치
npm install

# 플러그인 빌드
npm run build

# 개발 모드 (자동 재빌드)
npm run dev

# CLI 빌드
npm run build:cli

# 테스트 실행
npm test
```

### 플러그인 설치
1. 빌드 후 생성된 파일을 Obsidian 볼트의 플러그인 폴더에 복사:
   - `main.js`
   - `manifest.json`
   - `styles.css`

2. Obsidian 설정 → 커뮤니티 플러그인 → "Obsidian Vault LLM" 활성화

---

이 문서는 개발 진행 상황을 반영하여 지속적으로 업데이트됩니다.
